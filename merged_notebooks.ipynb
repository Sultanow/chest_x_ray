{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged notebook containing code and learnings from cxr project\n",
    "This notebook is meant as a starting point for the master thesis neural net. Ideas, learnings and code from previous notebooks is gathered and described, to summarize the current status of the project.\n",
    "\n",
    "## General components that can be kept\n",
    "*1. Preamble*<br>\n",
    "Imort of packages<br>\n",
    "Configuration variables like file paths, boolean switches, numeric settings, etc.\n",
    "\n",
    "*2. Getting the data*<br>\n",
    "Funciton definitions<br>\n",
    "*CHANGES NECESSARY* Read meta data + image files and convert it to a dataframe<br>\n",
    "*CHANGES NECESSARY* Unify labling of datasets<br>\n",
    "Train / Test / Val split<br>\n",
    "Shuffling data\n",
    "\n",
    "*3. Data preprocessing*<br>\n",
    "Image augmentation<br>\n",
    "Data generator class<br>\n",
    "\n",
    "*4. Model training*<br>\n",
    "**TODO: Think about a concept for comparing the nets**<br>\n",
    "Define neural net architecture<br>\n",
    "Model settings like learning rate reduction, early stopping, model save setting<br>\n",
    "Training the model\n",
    "\n",
    "*5. Model evaluation*<br>\n",
    "Check performance parameters like:\n",
    "- accuracy\n",
    "- loss\n",
    "- recall\n",
    "- f1\n",
    "\n",
    "Evaluate generalizability on validation data<br>\n",
    "Show confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, AveragePooling2D, Conv2D, MaxPool2D, Activation, GlobalAveragePooling2D, Lambda\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE=32\n",
    "CHANNELS=1\n",
    "USE_MASKS=False\n",
    "\n",
    "MODELSAVE = \"models/test_all_datasets\"\n",
    "\n",
    "if(USE_MASKS):\n",
    "    MODELSAVE += \"_w_masks.h5\"\n",
    "else:\n",
    "    MODELSAVE += \"_wo_masks.h5\"\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "DATASET_SOURCE = 1\n",
    "USE_AWS = False\n",
    "if USE_AWS:\n",
    "    DATSET_SOURCE = 2\n",
    "    \n",
    "base_path = ''\n",
    "\n",
    "if DATASET_SOURCE == 0:      # local\n",
    "    base_path = '/mnt/f/DatasetsChestXRay/'\n",
    "elif DATASET_SOURCE == 1:    # oth amberg\n",
    "    base_path = '/home/9424/NAVARA/masterarbeit/datasets/'\n",
    "elif DATASET_SOURCE == 2:    # aws\n",
    "    base_path = ''\n",
    "\n",
    "datasets = {\n",
    "    'padchest': 'BIMCV-PadChest/',\n",
    "    'cxr14': 'ChestX-ray14_NationalInstituesofHealthClinicalCenter/',\n",
    "    'chexpert': 'CheXpert/',\n",
    "    'kermany': 'KermanyChildStudy/',\n",
    "    'mimic': 'MIMIC-CXR/',\n",
    "    'openi': 'Open-i_IndianaUniversityNetworkforPatientCare/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset parent class\n",
    "A class that is suitable for creating a combined dataset consisting of different original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, dataset_dict=None):\n",
    "        self.df = None\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        \n",
    "        first = True\n",
    "        if dataset_dict is not None:\n",
    "            for dataset_name, dataset in dataset_dict.items():\n",
    "                if first:\n",
    "                    first = False\n",
    "                    self.df = dataset.df\n",
    "                else:\n",
    "                    self.df = pd.concat([self.df, dataset.df], ignore_index=True)\n",
    "\n",
    "    def train_test_split(self, test_size=0.2, random_state=200):\n",
    "        self.train, self.test = train_test_split(self.df, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    def stratified_train_test_split(self, test_size=0.2, random_state=200, stratify_column='dataset'):\n",
    "        self.train, self.test = train_test_split(self.df,\n",
    "                                                 test_size=test_size,\n",
    "                                                 random_state=random_state,\n",
    "                                                 stratify=None if stratify_column is None else self.df[stratify_column])\n",
    "\n",
    "    def shuffle(self, random_state=200):\n",
    "        self.df = self.df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        self.train = self.train.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        self.test = self.test.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset child class\n",
    "A class that is used to retrieve data from the original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class original_dataset(dataset):\n",
    "    def __init__(self, name, folder):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.folder = folder\n",
    "        self.path = base_path + self.folder\n",
    "        self.df = self.create_dataset()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        # initialize dataframe\n",
    "        data = {}\n",
    "        i = 0\n",
    "        # iterate over folders and files\n",
    "        for label_folder in ['normal/', 'pneumonia/']:\n",
    "            for filename in os.listdir(self.path + label_folder):\n",
    "                img_path = self.path + label_folder + str(filename)\n",
    "                label_pneumonia = 0\n",
    "                if label_folder == 'pneumonia/':\n",
    "                    label_pneumonia = 1\n",
    "                data[i] = {'img': img_path, 'dataset': self.name, 'label_pneumonia': label_pneumonia, 'label_viral': None, 'label_covid': None}\n",
    "                i = i + 1\n",
    "\n",
    "        print(\"Successfully created \" + self.name + \" dataset\")\n",
    "        return pd.DataFrame.from_dict(data, \"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created padchest dataset\n",
      "Successfully created cxr14 dataset\n",
      "Successfully created chexpert dataset\n",
      "Successfully created kermany dataset\n",
      "Successfully created mimic dataset\n",
      "Successfully created openi dataset\n"
     ]
    }
   ],
   "source": [
    "# dictionary of all datasets\n",
    "dataset_dict = {}\n",
    "\n",
    "# load all datasets and execute a train test split\n",
    "for dataset_name, folder_name in datasets.items():\n",
    "    tmp_dataset = original_dataset(dataset_name, folder_name)\n",
    "    tmp_dataset.train_test_split()\n",
    "    tmp_dataset.shuffle()\n",
    "    dataset_dict[dataset_name] = tmp_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_pneumonia</th>\n",
       "      <th>label_viral</th>\n",
       "      <th>label_covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person492_bacteria_2085.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person779_bacteria_2683.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1589_virus_2763.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1580_virus_2739.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/normal/NORMAL2-IM-1371-0001.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                img  \\\n",
       "0  /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person492_bacteria_2085.jpeg   \n",
       "1  /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person779_bacteria_2683.jpeg   \n",
       "2    /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1589_virus_2763.jpeg   \n",
       "3    /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1580_virus_2739.jpeg   \n",
       "4        /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/normal/NORMAL2-IM-1371-0001.jpeg   \n",
       "\n",
       "   dataset  label_pneumonia label_viral label_covid  \n",
       "0  kermany                1        None        None  \n",
       "1  kermany                1        None        None  \n",
       "2  kermany                1        None        None  \n",
       "3  kermany                1        None        None  \n",
       "4  kermany                0        None        None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['kermany'].df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label_pneumonia</th>\n",
       "      <th>label_viral</th>\n",
       "      <th>label_covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1449_virus_2476.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person978_virus_1653.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1608_virus_2786.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person494_bacteria_2090.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1087_bacteria_3027.jpeg</td>\n",
       "      <td>kermany</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 img  \\\n",
       "0     /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1449_virus_2476.jpeg   \n",
       "1      /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person978_virus_1653.jpeg   \n",
       "2     /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1608_virus_2786.jpeg   \n",
       "3   /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person494_bacteria_2090.jpeg   \n",
       "4  /home/9424/NAVARA/masterarbeit/datasets/KermanyChildStudy/pneumonia/person1087_bacteria_3027.jpeg   \n",
       "\n",
       "   dataset  label_pneumonia label_viral label_covid  \n",
       "0  kermany                1        None        None  \n",
       "1  kermany                1        None        None  \n",
       "2  kermany                1        None        None  \n",
       "3  kermany                1        None        None  \n",
       "4  kermany                1        None        None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['kermany'].test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "aug = iaa.AllChannelsCLAHE(clip_limit=(0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySigmoidContrast(img):\n",
    "    img = aug.augment_image(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "sometimes25 = lambda aug: iaa.Sometimes(0.25, aug)\n",
    "\n",
    "seq_img = iaa.Sequential([\n",
    "    # sometimes(iaa.SigmoidContrast(gain=(3, 10), cutoff=(0.4, 0.6))),\n",
    "    sometimes(iaa.Affine(\n",
    "            rotate=(-10, 10),\n",
    "            shear=(-10,10),\n",
    "            scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "            translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}\n",
    "    )),\n",
    "    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.15))),\n",
    "], random_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, df, augment, img_size, batch_size=8, shuffle=True, useMasks=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.indices = self.df.index.tolist()\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "        self.useMasks = useMasks\n",
    "        self.error_count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.indices)/ self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in indexes]\n",
    "        X, y = self.__get_data(batch)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X_img_paths = self.df['img']\n",
    "        # X_mask_paths = self.df['mask']\n",
    "        X_imgs = []\n",
    "        # X_masks = []\n",
    "        Y = self.df['label_pneumonia'].iloc[batch].to_numpy()\n",
    "\n",
    "        for i in range(0, len(batch)):\n",
    "            try:\n",
    "                img = cv2.resize(cv2.imread(str(X_img_paths.iloc[batch[i]]), cv2.IMREAD_GRAYSCALE), (self.img_size, self.img_size))\n",
    "                img = img.reshape(img.shape[0], img.shape[1],1)\n",
    "            except Exception as e:\n",
    "                self.error_count = self.error_count + 1\n",
    "                print('ERROR during cv2.resize, image was: ' + str(X_img_paths.iloc[batch[i]]))\n",
    "                print(str(e))\n",
    "            \n",
    "            if not self.useMasks:\n",
    "                if(self.augment is True):\n",
    "                    seq_img_i = seq_img.to_deterministic()   \n",
    "                    img = seq_img_i.augment_image(img)\n",
    "                # else img remains untouched\n",
    "            else:\n",
    "                try:\n",
    "                    mask = cv2.imread(str(X_mask_paths.iloc[batch[i]]), cv2.IMREAD_GRAYSCALE)\n",
    "                    #mask = cv2.resize(mask, (self.img_size, self.img_size))\n",
    "                    mask = cv2.threshold(mask,0,1,cv2.THRESH_BINARY)[1]\n",
    "                except Exception as e:\n",
    "                    self.error_count = self.error_count + 1\n",
    "                    print(str(e))\n",
    "                # Multiply binarized mask on grayscaled version of the x-ray. \n",
    "                # So only the lung will remain, everything else will be set to 0\n",
    "                try:\n",
    "                    mask = mask.reshape(IMG_SIZE,IMG_SIZE,1)\n",
    "                except:\n",
    "                    self.error_count = self.error_count + 1\n",
    "                    print(\"i\", i)\n",
    "                    print(str(X_mask_paths.iloc[batch[i]]))\n",
    "                cutout_img = img * mask\n",
    "                cutout_img = cutout_img.reshape(IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "                if(self.augment is True):\n",
    "                    seq_img_i = seq_img.to_deterministic()   \n",
    "                    img = seq_img_i.augment_image(img) #### img\n",
    "                    cutout_img = seq_img_i.augment_image(cutout_img) #### cutout\n",
    "\n",
    "                img = np.dstack([img, cutout_img])\n",
    "                mask = mask / 255.0\n",
    "\n",
    "            # normalize\n",
    "            img = img / 255.0\n",
    "\n",
    "            X_imgs.append(img)\n",
    "\n",
    "        return X_imgs, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = Sequential()\n",
    "    # Conv block1 = cb0\n",
    "    model.add(Conv2D(3, (1,1), padding=\"same\",activation=\"relu\", input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS)))\n",
    "    model.add(Conv2D(32, (3,3), padding = \"same\", activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS), name=\"cb0_conv0\"))\n",
    "    model.add(Conv2D(32, (3,3), padding = \"same\", activation='relu', name=\"cb0_conv1\"))\n",
    "    model.add(MaxPool2D(pool_size = (3, 3), name=\"cb0_maxpool\"))\n",
    "    model.add(Dropout(0.2, name=\"cb0_dropout\"))\n",
    "\n",
    "    # Conv block2 = cb1\n",
    "    model.add(Conv2D(64, (3,3), padding = \"same\", activation='relu', name=\"cb1_conv0\"))\n",
    "    model.add(Conv2D(64, (5,5), strides=(2,2), padding = \"same\", activation='relu', name=\"cb1_conv1\"))\n",
    "    model.add(Conv2D(64, (3,3), padding = \"same\", activation='relu', name=\"cb1_conv2\"))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), name=\"cb1_maxpool\"))\n",
    "    model.add(Dropout(0.2, name=\"cb1_dropout\"))\n",
    "\n",
    "    # Dense block = db\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(1024, activation='relu', name=\"db_dense1024\"))\n",
    "#     model.add(Dense(512, activation='relu', name=\"db_dense512\"))\n",
    "#     model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu', name=\"db_dense256\"))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid', name=\"db_dense1\"))\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate=0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf5 = StratifiedKFold(n_splits=5, random_state=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.8,\n",
    "    patience=3,\n",
    "    verbose=2, # verbose=1 results in a warning, use verbose=2 (bit less info) or verbose=0 (no info) instead\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "mcp_save = ModelCheckpoint(MODELSAVE, save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dataset(dataset_dict).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63499"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run = 1\n",
    "val_scores = []\n",
    "f1_scores = []\n",
    "histories = []\n",
    "best_f1_model = None\n",
    "highestF1 = 0\n",
    "EPOCHS = 12 # 12\n",
    "\n",
    "#df_train = dataset_dict['kermany'].train.head(200)\n",
    "\n",
    "\n",
    "# hier das .label_pneumonia ändern in die kombinierte Variable\n",
    "for train_index, val_index in kf5.split(df_train, df_train.label_pneumonia):\n",
    "    print(\"Fold Nr.: \", run)\n",
    "    df_fold_train = df_train.iloc[train_index]\n",
    "    df_fold_val = df_train.iloc[val_index]\n",
    "\n",
    "    df_fold_train.reset_index(drop=True, inplace=True)\n",
    "    df_fold_val.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    # print('=== Train Data: ===')\n",
    "    # print('Normal:', len(df_fold_train[df_fold_train['label'] == 0]))\n",
    "    # print('Pneumonia:', len(df_fold_train[df_fold_train['label'] == 1]))\n",
    "    # print('|---> Bacterial:', len(df_fold_train[df_fold_train['label2'] == 'bacteria']))\n",
    "    # print('|---> Viral:', len(df_fold_train[df_fold_train['label2'] == 'virus']))\n",
    "    # print('    |---> Covid:', len(df_fold_train[df_fold_train['label3'] == 'covid19']))\n",
    "    \n",
    "    # print('=== Val Data: ===')\n",
    "    # print('Normal:', len(df_fold_val[df_fold_val['label'] == 0]))\n",
    "    # print('Pneumonia:', len(df_fold_val[df_fold_val['label'] == 1]))\n",
    "    # print('|---> Bacterial:', len(df_fold_val[df_fold_val['label2'] == 'bacteria']))\n",
    "    # print('|---> Viral:', len(df_fold_val[df_fold_val['label2'] == 'virus']))\n",
    "    # print('    |---> Covid:', len(df_fold_val[df_fold_val['label3'] == 'covid19']))\n",
    "    \n",
    "    train_generator = DataGenerator(df_fold_train, True, IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False, useMasks=USE_MASKS)\n",
    "    val_generator = DataGenerator(df_fold_val, False, IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False, useMasks=USE_MASKS)\n",
    "    \n",
    "\n",
    "    # Adjust class_weights since we're dealing with imbalanced data here\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(df_fold_train['label_pneumonia']), df_fold_train['label_pneumonia'])\n",
    "    class_weights = {0: class_weights[0], 1: class_weights[1]}\n",
    "    print(\"Class weights: \", class_weights)\n",
    "\n",
    "    model = getModel()\n",
    "    history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=len(val_generator),\n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    epochs=EPOCHS,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[mcp_save, learning_rate_reduction], #mcp_save, learning_rate_reduction]],\n",
    "                    verbose = True)\n",
    "\n",
    "    predictions = model.predict(val_generator)\n",
    "    y_pred = (predictions > 0.5).astype('uint8')\n",
    "\n",
    "    histories.append(history)\n",
    "\n",
    "    f1 = f1_score(y_pred, df_fold_val.label_pneumonia)\n",
    "\n",
    "    if(highestF1 == 0):\n",
    "        highestF1 = f1\n",
    "        best_f1_model = model\n",
    "    else:\n",
    "        if(f1 > highestF1):\n",
    "            highestF1 = f1\n",
    "            best_f1_model = model\n",
    "\n",
    "    val_acc = accuracy_score(y_pred , df_fold_val.label_pneumonia)\n",
    "\n",
    "    f1_scores.append(f1)\n",
    "    val_scores.append(val_acc)\n",
    "    \n",
    "    run += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(f1_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(val_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to the best f1 model\n",
    "model = best_f1_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dataset_dict['kermany'].test.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(df_test, False, IMG_SIZE, batch_size=32, shuffle=False, useMasks=USE_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_generator.__getitem__(0)[0][0][:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test pneumonia cases: \", len(df_test[df_test['label_pneumonia'] == 1]))\n",
    "print(\"Test normal cases: \", len(df_test[df_test['label_pneumonia'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = test_generator.df.label_pneumonia # we can do this since we do not shuffle in test_generator ;)\n",
    "y_pred = predictions > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = []\n",
    "\n",
    "for x in y_pred:\n",
    "    if x == 0:\n",
    "        y_pred_classes.append(\"NORMAL\")\n",
    "    else:\n",
    "        y_pred_classes.append(\"PNEUMONIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_classes = []\n",
    "\n",
    "for x in y_true:\n",
    "    if x == 0:\n",
    "        y_true_classes.append(\"NORMAL\")\n",
    "    else:\n",
    "        y_true_classes.append(\"PNEUMONIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true_classes, y_pred_classes) #, target_names=[\"NORMAL\", \"PNEUMONIA\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt='g', cmap='Blues') #annot=True to annotate cells\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('normal vs. pneumonia (w/o masks)')\n",
    "ax.xaxis.set_ticklabels(['Normal', 'Pneumonia'])\n",
    "ax.yaxis.set_ticklabels(['Normal', 'Pneumonia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
